{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "299b1169",
   "metadata": {},
   "source": [
    "# (ARCHIVED) Testing and Debugging in Notebook\n",
    "This notebook is for internal experimentation and has been archived. The\n",
    "public repository does not require it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230f51a5",
   "metadata": {},
   "source": [
    "## Setup Testing Environment\n",
    "Install and import `unittest` or `pytest`, configure the test directory, and show how to write a minimal module to be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363df6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python3 -m pip install pytest >/dev/null 2>&1 || true\n",
    "\n",
    "# create a simple module to test\n",
    "cat <<'EOF' > /workspaces/NoLimit_benchmarks/benchmarks/sample_module.py\n",
    "\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "EOF\n",
    "\n",
    "# create a tests directory with a test file\n",
    "mkdir -p /workspaces/NoLimit_benchmarks/benchmarks/tests\n",
    "cat <<'EOF' > /workspaces/NoLimit_benchmarks/benchmarks/tests/test_sample_module.py\n",
    "from benchmarks.sample_module import add\n",
    "\n",
    "def test_add():\n",
    "    assert add(2, 3) == 5\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d717ae",
   "metadata": {},
   "source": [
    "## Write a Simple Test Case\n",
    "Create a test function or class using `unittest.TestCase` or a `pytest`-style function, including assertions and expected outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0235102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test file already created above; display contents\n",
    "!sed -n '1,200p' benchmarks/tests/test_sample_module.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1697f708",
   "metadata": {},
   "source": [
    "## Run Tests from Notebook\n",
    "Invoke the test runner from within the notebook using `!pytest` or `unittest.main()` and display output in the output pane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c89df44",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest -q benchmarks/tests -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d41c9e",
   "metadata": {},
   "source": [
    "## Debug Failing Tests with `pdb`\n",
    "Introduce `import pdb; pdb.set_trace()` or use `pytest --pdb` to step through failing tests interactively in the notebook cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dd2de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# modify test to fail and add pdb\n",
    "cat <<'EOF' > benchmarks/tests/test_sample_module.py\n",
    "import pdb\n",
    "from benchmarks.sample_module import add\n",
    "\n",
    "def test_add():\n",
    "    pdb.set_trace()\n",
    "    assert add(2, 3) == 6  # wrong on purpose\n",
    "EOF\n",
    "\n",
    "# run pytest with pdb enabled\n",
    "pytest -q --pdb benchmarks/tests || true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77f3c12",
   "metadata": {},
   "source": [
    "## Use VSCode Debugger Integration\n",
    "Demonstrate launching the VSCode debugger on a test file, setting breakpoints, and interpreting the output in the integrated terminal and debug console.\n",
    "\n",
    "1. Open the test file in VSCode.\n",
    "2. Add a breakpoint by clicking in the gutter.\n",
    "3. Use the \"Run and Debug\" panel or `F5` to start debugging the test.\n",
    "4. Inspect variables in the debug console when execution pauses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5500986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Debug the Benchmark Script\n",
    "Example of adding a breakpoint into `benchmarks/run_benchmarks.py` and running it interactively using `pdb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667c8486",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# insert a pdb breakpoint in the script\n",
    "sed -i '1iimport pdb; pdb.set_trace()\\n' benchmarks/run_benchmarks.py\n",
    "\n",
    "# run the script and let the notebook capture the pdb session\n",
    "python3 benchmarks/run_benchmarks.py || true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd89ac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewrite README.md to production-ready content\n",
    "readme = '''# NoLimit-Benchmarks\n",
    "\n",
    "**Public performance benchmarks for the NoLimit lossless compression system.**\n",
    "\n",
    "Transparent, reproducible results showing superior ratios and speed across 1D–5D data.\n",
    "\n",
    "**Latest Results (February 2026)**\n",
    "\n",
    "| Dataset                  | NoLimit Ratio | ZSTD Ratio | NoLimit Speed | Self-Optimizing |\n",
    "|--------------------------|---------------|------------|---------------|-----------------|\n",
    "| 1D Text (1K tokens)      | **82.3%**     | 71.2%      | **12 ms**     | Yes             |\n",
    "| 2D Image (512×512)       | **88.7%**     | 79.4%      | **45 ms**     | Yes             |\n",
    "| 3D Video (1080p)         | **91.5%**     | 84.1%      | **78 ms**     | Yes             |\n",
    "| 4D Light Field           | **93.2%**     | 87.6%      | **112 ms**    | Yes             |\n",
    "| **5D Light Field Video** | **95.2%**     | 89.3%      | **156 ms**    | Yes             |\n",
    "\n",
    "**Key Highlights**\n",
    "- Entropy-guided self-optimization automatically adapts depth\n",
    "- Native multi-dimensional support (1D → 5D)\n",
    "- Fully reproducible with public datasets\n",
    "\n",
    "**Run the benchmarks yourself**\n",
    "```bash\n",
    " git clone https://github.com/BoundlessAI/NoLimit-Benchmarks\n",
    " cd NoLimit-Benchmarks\n",
    " python -m venv venv && source venv/bin/activate\n",
    " pip install -r requirements.txt\n",
    " python benchmarks/run_benchmarks.py\n",
    "```\n",
    "\n",
    "Note: This repository contains only benchmark harnesses and safe pre-compiled stubs. The full proprietary NoLimit Engine (including the complete algorithm) is available under commercial license.\n",
    "\n",
    "Enterprise licensing & private source: enterprise@boundless.ai\n",
    "\n",
    "Made with ❤️ by Ian Knotts @ Boundless AI Dev\n",
    "'''\n",
    "with open('/workspaces/NoLimit_benchmarks/README.md','w') as f:\n",
    "    f.write(readme)\n",
    "print('README rewritten')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
